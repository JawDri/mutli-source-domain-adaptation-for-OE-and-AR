{"cells":[{"cell_type":"markdown","metadata":{"id":"f260223a"},"source":["<a id=\"section-one\"></a>\n","## 1 Import Libraries and download data"],"id":"f260223a"},{"cell_type":"markdown","metadata":{"id":"dd9d1294"},"source":["<a id=\"define-device\"></a>\n","### 1.2 Define device"],"id":"dd9d1294"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21107,"status":"ok","timestamp":1698091245481,"user":{"displayName":"J D","userId":"03026085817368399956"},"user_tz":240},"id":"FqEDEQQJpwtB","outputId":"38b2f640-65f1-4173-edd7-f6a1e28a70f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"FqEDEQQJpwtB"},{"cell_type":"markdown","metadata":{"id":"dgKhIfA6koN3"},"source":["#OE 2 unbalanced labels"],"id":"dgKhIfA6koN3"},{"cell_type":"code","execution_count":3,"metadata":{"id":"oaAL6_V_kn1J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698091440439,"user_tz":240,"elapsed":8487,"user":{"displayName":"J D","userId":"03026085817368399956"}},"outputId":"1c62d8ae-483e-4140-887c-59573048d571"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=1\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=1\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=1\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","Data_S3 = Final_Data_S1.drop(['time'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:1045]\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:300]\n","frames = [Data_0, Data_2]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Data_S3[Data_S3.labels == 0].iloc[:1045]\n","Data_2 = Data_S3[Data_S3.labels == 1].iloc[:300]\n","\n","frames = [Data_0, Data_2]\n","Data_S3 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:772]\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:200]\n","frames = [Data_0, Data_2]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Data_S3 = Data_S3.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[500:,:]\n","Source_test_1 = Final_Data_S1.iloc[:500,:]\n","\n","Source_train_2 = Data_S3.iloc[500: ,[0,1,2,4,5,6,8,9,10,13]]\n","Source_test_2 = Data_S3.iloc[:500,[0,1,2,4,5,6,8,9,10,13]]\n","\n","Target_train = Final_Data_S2.iloc[500:,:]\n","Target_test = Final_Data_S2.iloc[:500,:]\n","\n","\n","\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"oaAL6_V_kn1J"},{"cell_type":"markdown","metadata":{"id":"H5kJaPJNi3uc"},"source":["#OE 3 unbalanced labels"],"id":"H5kJaPJNi3uc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5974,"status":"ok","timestamp":1698086026237,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"},"user_tz":240},"id":"Y6s34COBi3MM","outputId":"bb4e9fed-3a28-4bce-cc64-0209e0941c87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=2\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=2\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=2\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","Data_S3 = Final_Data_S1.drop(['time'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:1045]\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:300]\n","Data_3 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:300]\n","frames = [Data_0, Data_2, Data_3]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Data_S3[Data_S3.labels == 0].iloc[:1045]\n","Data_2 = Data_S3[Data_S3.labels == 1].iloc[:300]\n","Data_3 = Data_S3[Data_S3.labels == 2].iloc[:300]\n","frames = [Data_0, Data_2, Data_3]\n","Data_S3 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:772]\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:200]\n","Data_3 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:200]\n","frames = [Data_0, Data_2, Data_3]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Data_S3 = Data_S3.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[500:,:]\n","Source_test_1 = Final_Data_S1.iloc[:500,:]\n","\n","Source_train_2 = Data_S3.iloc[500: ,[0,1,2,4,5,6,8,9,10,13]]\n","Source_test_2 = Data_S3.iloc[:500,[0,1,2,4,5,6,8,9,10,13]]\n","\n","Target_train = Final_Data_S2.iloc[500:,:]\n","Target_test = Final_Data_S2.iloc[:500,:]\n","\n","\n","\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"Y6s34COBi3MM"},{"cell_type":"markdown","metadata":{"id":"iHUKkTdKkJP2"},"source":["#OE balanced 2 labels"],"id":"iHUKkTdKkJP2"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5238,"status":"ok","timestamp":1698086732949,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"},"user_tz":240},"id":"qm7aAMUikGkX","outputId":"78d78784-e167-48f3-b433-36d27759fb00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=1\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=1\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=1\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","Data_S3 = Final_Data_S1.drop(['time'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Data_S3 = Data_S3.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[500:,:]\n","Source_test_1 = Final_Data_S1.iloc[:500,:]\n","\n","Source_train_2 = Data_S3.iloc[500: ,[0,1,2,4,5,6,8,9,10,13]]\n","Source_test_2 = Data_S3.iloc[:500,[0,1,2,4,5,6,8,9,10,13]]\n","\n","Target_train = Final_Data_S2.iloc[500:,:]\n","Target_test = Final_Data_S2.iloc[:500,:]\n","\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"qm7aAMUikGkX"},{"cell_type":"markdown","metadata":{"id":"t_6Ru51m_deH"},"source":["#OE 3 labels balanced"],"id":"t_6Ru51m_deH"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4659,"status":"ok","timestamp":1698085124755,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"},"user_tz":240},"id":"La67l8Ng-xmm","outputId":"dab38afc-6761-4c88-9a08-546e03d6d9aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=2\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=2\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=2\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","Data_S3 = Final_Data_S1.drop(['time'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Data_S3 = Data_S3.sample(frac = 1)\n","\n","\n","\n","Source_train_1 = Final_Data_S1.iloc[500:,:]\n","Source_test_1 = Final_Data_S1.iloc[:500,:]\n","\n","Source_train_2 = Data_S3.iloc[500: ,[0,1,2,4,5,6,8,9,10,13]]\n","Source_test_2 = Data_S3.iloc[:500,[0,1,2,4,5,6,8,9,10,13]]\n","\n","Target_train = Final_Data_S2.iloc[500:,:]\n","Target_test = Final_Data_S2.iloc[:500,:]\n","\n","#Source_train = poison_data(Source_train, 0.1)\n","#Target_train = poison_data(Target_train, 0.1)\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"La67l8Ng-xmm"},{"cell_type":"markdown","metadata":{"id":"aW1uEd59_iOv"},"source":["#Load Data AR: 3 Activities"],"id":"aW1uEd59_iOv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyaalwUr-65K"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","'''\n","!wget http://casas.wsu.edu/datasets/hh101.zip\n","!wget http://casas.wsu.edu/datasets/hh105.zip\n","!wget http://casas.wsu.edu/datasets/hh104.zip\n","!wget http://casas.wsu.edu/datasets/hh103.zip\n","!unzip '/content/hh105.zip'\n","!unzip '/content/hh104.zip'\n","!unzip /content/hh101.zip\n","!unzip /content/hh103.zip\n","'''\n","\n","Final_Data_S1 = pd.read_csv(\"/content/hh101/hh101.ann.features.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/hh105/hh105.ann.features.csv\")\n","Final_Data_S3 = pd.read_csv(\"/content/hh104/hh104.ann.features.csv\")\n","Final_Data_S4 = pd.read_csv(\"/content/hh103/hh103.ann.features.csv\")\n","Final_Data_S1.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S2.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S3.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S4.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","\n","Final_Data_S1 = Final_Data_S1.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S2 = Final_Data_S2.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","Final_Data_S3 = Final_Data_S3.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S4 = Final_Data_S4.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","\n","\n","Final_Data_S1 = Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S2 = Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S3 = Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S4 = Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner', 'Watch_TV', 'Toilet' ])]\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner'])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Toilet'])].index,'labels']=2\n","\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner'])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Toilet'])].index,'labels']=2\n","\n","\n","\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner'])].index,'labels']=1\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Toilet'])].index,'labels']=2\n","\n","\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner'])].index,'labels']=1\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Toilet'])].index,'labels']=2\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:2000]#2000#4000\n","Data_1 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:2000]#4000\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:2000]#2000\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:1000]#1000#2000\n","Data_1 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:1000]#2000\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:1000]#\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S3[Final_Data_S3.labels == 0].iloc[:2000]#2000#4000\n","Data_1 = Final_Data_S3[Final_Data_S3.labels == 1].iloc[:2000]#4000\n","Data_2 = Final_Data_S3[Final_Data_S3.labels == 2].iloc[:2000]#2000\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S3 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S4[Final_Data_S4.labels == 0].iloc[:2000]#4000#2000\n","Data_1 = Final_Data_S4[Final_Data_S4.labels == 1].iloc[:2000]#2000\n","Data_2 = Final_Data_S4[Final_Data_S4.labels == 2].iloc[:2000]#\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S4 = pd.concat(frames)\n","\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Final_Data_S3 = Final_Data_S3.sample(frac = 1)\n","Final_Data_S4 = Final_Data_S4.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[200:,:]#1000#\n","Source_test_1 = Final_Data_S1.iloc[:200,:]\n","\n","Source_train_2 = Final_Data_S3.iloc[200:,:]#1000#\n","Source_test_2 = Final_Data_S3.iloc[:200,:]\n","\n","Source_train_3 = Final_Data_S4.iloc[200:,:]#1000#\n","Source_test_3 = Final_Data_S4.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:,:]#500#\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","#Source_train_3.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_3.csv', index=False)\n","#Source_test_3.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_3.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"wyaalwUr-65K"},{"cell_type":"markdown","metadata":{"id":"hRCYevEuAB4A"},"source":["#Load Data AR: 5 Activities"],"id":"hRCYevEuAB4A"},{"cell_type":"code","execution_count":null,"metadata":{"id":"01hqvUJ9AGiO"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","'''\n","!wget http://casas.wsu.edu/datasets/hh101.zip\n","!wget http://casas.wsu.edu/datasets/hh105.zip\n","!wget http://casas.wsu.edu/datasets/hh104.zip\n","!wget http://casas.wsu.edu/datasets/hh103.zip\n","!unzip '/content/hh105.zip'\n","!unzip '/content/hh104.zip'\n","!unzip /content/hh101.zip\n","!unzip /content/hh103.zip'''\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/hh101/hh101.ann.features.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/hh105/hh105.ann.features.csv\")\n","Final_Data_S3 = pd.read_csv(\"/content/hh104/hh104.ann.features.csv\")\n","Final_Data_S4 = pd.read_csv(\"/content/hh103/hh103.ann.features.csv\")\n","Final_Data_S1.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S2.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S3.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S4.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","\n","Final_Data_S1 = Final_Data_S1.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S2 = Final_Data_S2.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","Final_Data_S3 = Final_Data_S3.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S4 = Final_Data_S4.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","\n","\n","Final_Data_S1 = Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S2 = Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S3 = Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S4 = Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:2000]#2000#4000\n","Data_1 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:2000]#4000\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:2000]#2000\n","Data_3 = Final_Data_S1[Final_Data_S1.labels == 3].iloc[:2000]#2000\n","Data_4 = Final_Data_S1[Final_Data_S1.labels == 4].iloc[:2000]#\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:1000]#1000#2000\n","Data_1 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:1000]#2000\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:1000]#\n","Data_3 = Final_Data_S2[Final_Data_S2.labels == 3].iloc[:1000]#1000\n","Data_4 = Final_Data_S2[Final_Data_S2.labels == 4].iloc[:1000]#\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S3[Final_Data_S3.labels == 0].iloc[:2000]#2000#4000\n","Data_1 = Final_Data_S3[Final_Data_S3.labels == 1].iloc[:2000]#4000\n","Data_2 = Final_Data_S3[Final_Data_S3.labels == 2].iloc[:2000]#2000\n","Data_3 = Final_Data_S3[Final_Data_S3.labels == 3].iloc[:2000]#2000\n","Data_4 = Final_Data_S3[Final_Data_S3.labels == 4].iloc[:2000]#\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S3 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S4[Final_Data_S4.labels == 0].iloc[:2000]#4000#2000\n","Data_1 = Final_Data_S4[Final_Data_S4.labels == 1].iloc[:2000]#2000\n","Data_2 = Final_Data_S4[Final_Data_S4.labels == 2].iloc[:2000]#\n","Data_3 = Final_Data_S4[Final_Data_S4.labels == 3].iloc[:2000]#2000\n","Data_4 = Final_Data_S4[Final_Data_S4.labels == 4].iloc[:2000]#\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S4 = pd.concat(frames)\n","\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Final_Data_S3 = Final_Data_S3.sample(frac = 1)\n","Final_Data_S4 = Final_Data_S4.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[200:,:]#1000#\n","Source_test_1 = Final_Data_S1.iloc[:200,:]\n","\n","Source_train_2 = Final_Data_S3.iloc[200:,:]#1000#\n","Source_test_2 = Final_Data_S3.iloc[:200,:]\n","\n","Source_train_3 = Final_Data_S4.iloc[200:,:]#1000#\n","Source_test_3 = Final_Data_S4.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:,:]#500#\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_2.csv', index=False)\n","\n","#Source_train_3.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_train_3.csv', index=False)\n","#Source_test_3.to_csv('/content/drive/MyDrive/MetaTeacher/data/Source_test_3.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/MetaTeacher/data/Target_test.csv', index=False)"],"id":"01hqvUJ9AGiO"},{"cell_type":"code","execution_count":4,"metadata":{"id":"rB2HRmYbE8G3","executionInfo":{"status":"ok","timestamp":1698091495067,"user_tz":240,"elapsed":143,"user":{"displayName":"J D","userId":"03026085817368399956"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/MetaTeacher')"],"id":"rB2HRmYbE8G3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uILJrneVHn9t"},"outputs":[],"source":["!python train_source.py"],"id":"uILJrneVHn9t"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100760,"status":"ok","timestamp":1698091712173,"user":{"displayName":"J D","userId":"03026085817368399956"},"user_tz":240},"id":"ROBK6-6QRlkw","outputId":"525baad5-67ec-4610-e29d-a71c5e91996b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/models/teacher/resnet20.py:36: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","  init.kaiming_normal(m.weight)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/4]adapter Loss: 0.6399\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.147 (0.147)\tLoss 0.6444 (0.6444)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.5500 (0.5584)\t\n","Test: [10/15]\tTime 0.004 (0.019)\tLoss 0.6455 (0.5824)\t\n","Acc: [tensor(77.4951)] F1score: 0.7884102844719582\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[1/4]adapter Loss: 0.0582\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.146 (0.146)\tLoss 0.6228 (0.6228)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.5225 (0.5332)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.6180 (0.5560)\t\n","Acc: [tensor(78.4736)] F1score: 0.7962511541468583\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[2/4]adapter Loss: 0.1046\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.151 (0.151)\tLoss 0.5729 (0.5729)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.4585 (0.4724)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.5580 (0.4920)\t\n","Acc: [tensor(83.1703)] F1score: 0.8309887162308837\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[3/4]adapter Loss: 0.1184\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.220 (0.220)\tLoss 0.5207 (0.5207)\t\n","Test: [5/15]\tTime 0.009 (0.046)\tLoss 0.4080 (0.4214)\t\n","Test: [10/15]\tTime 0.009 (0.030)\tLoss 0.5052 (0.4391)\t\n","Acc: [tensor(83.7573)] F1score: 0.83507181522856\n","--- adapter training cost 0.075 mins ---\n","\n","===> epoch: 1/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.139 (0.139)\tLoss 1.0435 (1.0435)\t\n","Test: [5/15]\tTime 0.007 (0.034)\tLoss 0.7862 (0.8218)\t\n","Test: [10/15]\tTime 0.005 (0.021)\tLoss 1.0606 (0.8396)\t\n","Acc: [tensor(81.4090)] F1score: 0.7615044828868527\n","[tensor(81.4090)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.159 (0.159)\tLoss 0.4930 (0.4930)\t\n","Test: [5/15]\tTime 0.008 (0.033)\tLoss 0.4802 (0.4482)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.2644 (0.3713)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.149 (0.149)\tLoss 8.0766 (8.0766)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 9.1205 (8.5378)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 8.2690 (8.6398)\t\n","Acc: [tensor(23.2877)] F1score: 0.24827611705765992\n","\n","===> epoch: 2/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.151 (0.151)\tLoss 1.0131 (1.0131)\t\n","Test: [5/15]\tTime 0.008 (0.033)\tLoss 0.7569 (0.7852)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9999 (0.8051)\t\n","Acc: [tensor(81.6047)] F1score: 0.7627561772865099\n","[tensor(81.6047)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.182 (0.182)\tLoss 0.4951 (0.4951)\t\n","Test: [5/15]\tTime 0.015 (0.041)\tLoss 0.4825 (0.4497)\t\n","Test: [10/15]\tTime 0.008 (0.026)\tLoss 0.2655 (0.3726)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.232 (0.232)\tLoss 0.6008 (0.6008)\t\n","Test: [5/15]\tTime 0.012 (0.051)\tLoss 0.5241 (0.5051)\t\n","Test: [10/15]\tTime 0.004 (0.030)\tLoss 0.5867 (0.5091)\t\n","Acc: [tensor(80.8219)] F1score: 0.7552604581878932\n","\n","===> epoch: 3/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.145 (0.145)\tLoss 1.0076 (1.0076)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.7736 (0.7919)\t\n","Test: [10/15]\tTime 0.005 (0.021)\tLoss 1.0221 (0.8138)\t\n","Acc: [tensor(82.3875)] F1score: 0.7752391962950039\n","[tensor(82.3875)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.157 (0.157)\tLoss 0.4977 (0.4977)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.4854 (0.4517)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.2668 (0.3742)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.152 (0.152)\tLoss 0.6586 (0.6586)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.5933 (0.4807)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.7300 (0.5147)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 4/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.186 (0.186)\tLoss 1.0392 (1.0392)\t\n","Test: [5/15]\tTime 0.009 (0.040)\tLoss 0.8097 (0.8351)\t\n","Test: [10/15]\tTime 0.006 (0.026)\tLoss 1.0711 (0.8526)\t\n","Acc: [tensor(81.6047)] F1score: 0.7699621379009327\n","[tensor(81.6047)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.221 (0.221)\tLoss 0.4999 (0.4999)\t\n","Test: [5/15]\tTime 0.013 (0.047)\tLoss 0.4879 (0.4534)\t\n","Test: [10/15]\tTime 0.006 (0.029)\tLoss 0.2680 (0.3756)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.202 (0.202)\tLoss 0.7165 (0.7165)\t\n","Test: [5/15]\tTime 0.009 (0.045)\tLoss 0.6579 (0.5195)\t\n","Test: [10/15]\tTime 0.006 (0.028)\tLoss 0.8105 (0.5611)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 5/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.147 (0.147)\tLoss 1.0070 (1.0070)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.7875 (0.8059)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 1.0196 (0.8235)\t\n","Acc: [tensor(82.1918)] F1score: 0.7805094230306013\n","[tensor(82.1918)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.173 (0.173)\tLoss 0.5015 (0.5015)\t\n","Test: [5/15]\tTime 0.007 (0.036)\tLoss 0.4898 (0.4545)\t\n","Test: [10/15]\tTime 0.004 (0.023)\tLoss 0.2692 (0.3767)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.158 (0.158)\tLoss 0.7309 (0.7309)\t\n","Test: [5/15]\tTime 0.007 (0.034)\tLoss 0.6738 (0.5292)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8304 (0.5726)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 6/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4716\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.149 (0.149)\tLoss 0.9866 (0.9866)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.7608 (0.7795)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 1.0011 (0.7996)\t\n","Acc: [tensor(82.1918)] F1score: 0.7783920194391869\n","--- adapter training cost 0.017 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.139 (0.139)\tLoss 0.9679 (0.9679)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.7411 (0.7745)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9778 (0.7891)\t\n","Acc: [tensor(81.8004)] F1score: 0.7689458442241541\n","[tensor(81.8004)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.145 (0.145)\tLoss 0.5041 (0.5041)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.4926 (0.4566)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.2705 (0.3783)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.144 (0.144)\tLoss 0.7293 (0.7293)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.6721 (0.5282)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8283 (0.5714)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 7/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4970\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.159 (0.159)\tLoss 0.9666 (0.9666)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.7385 (0.7591)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9647 (0.7785)\t\n","Acc: [tensor(81.9961)] F1score: 0.7725938614394413\n","--- adapter training cost 0.017 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.151 (0.151)\tLoss 0.9358 (0.9358)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.7185 (0.7351)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9353 (0.7545)\t\n","Acc: [tensor(82.5832)] F1score: 0.781113948994067\n","[tensor(82.5832)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.160 (0.160)\tLoss 0.5071 (0.5071)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.4959 (0.4591)\t\n","Test: [10/15]\tTime 0.006 (0.021)\tLoss 0.2717 (0.3802)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.162 (0.162)\tLoss 0.7291 (0.7291)\t\n","Test: [5/15]\tTime 0.007 (0.034)\tLoss 0.6718 (0.5281)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8281 (0.5712)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 8/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5018\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.194 (0.194)\tLoss 0.9417 (0.9417)\t\n","Test: [5/15]\tTime 0.009 (0.046)\tLoss 0.7257 (0.7504)\t\n","Test: [10/15]\tTime 0.007 (0.029)\tLoss 0.9470 (0.7665)\t\n","Acc: [tensor(82.3875)] F1score: 0.7797509830966378\n","--- adapter training cost 0.020 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.227 (0.227)\tLoss 0.8788 (0.8788)\t\n","Test: [5/15]\tTime 0.010 (0.049)\tLoss 0.6786 (0.7039)\t\n","Test: [10/15]\tTime 0.007 (0.031)\tLoss 0.8738 (0.7173)\t\n","Acc: [tensor(82.1918)] F1score: 0.7783920194391869\n","[tensor(82.1918)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.177 (0.177)\tLoss 0.5105 (0.5105)\t\n","Test: [5/15]\tTime 0.007 (0.037)\tLoss 0.4994 (0.4619)\t\n","Test: [10/15]\tTime 0.004 (0.023)\tLoss 0.2728 (0.3822)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.157 (0.157)\tLoss 0.7239 (0.7239)\t\n","Test: [5/15]\tTime 0.008 (0.033)\tLoss 0.6661 (0.5246)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8210 (0.5671)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 9/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4925\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.157 (0.157)\tLoss 0.9410 (0.9410)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.7230 (0.7515)\t\n","Test: [10/15]\tTime 0.007 (0.021)\tLoss 0.9565 (0.7666)\t\n","Acc: [tensor(81.8004)] F1score: 0.7665254411418663\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.143 (0.143)\tLoss 0.9014 (0.9014)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.6854 (0.7120)\t\n","Test: [10/15]\tTime 0.004 (0.019)\tLoss 0.9068 (0.7280)\t\n","Acc: [tensor(82.3875)] F1score: 0.7703700409179861\n","[tensor(82.3875)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.159 (0.159)\tLoss 0.5133 (0.5133)\t\n","Test: [5/15]\tTime 0.007 (0.036)\tLoss 0.5024 (0.4641)\t\n","Test: [10/15]\tTime 0.004 (0.022)\tLoss 0.2740 (0.3839)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.150 (0.150)\tLoss 0.7162 (0.7162)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.6577 (0.5195)\t\n","Test: [10/15]\tTime 0.007 (0.021)\tLoss 0.8106 (0.5611)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 10/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.7032\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.142 (0.142)\tLoss 0.9114 (0.9114)\t\n","Test: [5/15]\tTime 0.008 (0.032)\tLoss 0.7059 (0.7263)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9154 (0.7422)\t\n","Acc: [tensor(82.5832)] F1score: 0.7832625245064577\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.153 (0.153)\tLoss 0.8865 (0.8865)\t\n","Test: [5/15]\tTime 0.009 (0.034)\tLoss 0.6697 (0.6988)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8735 (0.7134)\t\n","Acc: [tensor(82.1918)] F1score: 0.7761951613310125\n","[tensor(82.1918)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.151 (0.151)\tLoss 0.5153 (0.5153)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.5046 (0.4657)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.2750 (0.3852)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.148 (0.148)\tLoss 0.7093 (0.7093)\t\n","Test: [5/15]\tTime 0.009 (0.032)\tLoss 0.6501 (0.5150)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8011 (0.5556)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 11/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.6721\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.146 (0.146)\tLoss 0.9147 (0.9147)\t\n","Test: [5/15]\tTime 0.008 (0.031)\tLoss 0.7042 (0.7275)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.9107 (0.7426)\t\n","Acc: [tensor(82.5832)] F1score: 0.7832625245064577\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.232 (0.232)\tLoss 0.9334 (0.9334)\t\n","Test: [5/15]\tTime 0.009 (0.048)\tLoss 0.7182 (0.7452)\t\n","Test: [10/15]\tTime 0.006 (0.030)\tLoss 0.9562 (0.7604)\t\n","Acc: [tensor(82.3875)] F1score: 0.7703700409179861\n","[tensor(82.3875)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.220 (0.220)\tLoss 0.5186 (0.5186)\t\n","Test: [5/15]\tTime 0.010 (0.047)\tLoss 0.5081 (0.4684)\t\n","Test: [10/15]\tTime 0.006 (0.029)\tLoss 0.2761 (0.3872)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.246 (0.246)\tLoss 0.7008 (0.7008)\t\n","Test: [5/15]\tTime 0.009 (0.049)\tLoss 0.6406 (0.5093)\t\n","Test: [10/15]\tTime 0.006 (0.030)\tLoss 0.7895 (0.5488)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 12/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5751\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.147 (0.147)\tLoss 0.8991 (0.8991)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.6904 (0.7115)\t\n","Test: [10/15]\tTime 0.004 (0.019)\tLoss 0.9106 (0.7284)\t\n","Acc: [tensor(81.8004)] F1score: 0.7665254411418663\n","--- adapter training cost 0.018 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.144 (0.144)\tLoss 0.8750 (0.8750)\t\n","Test: [5/15]\tTime 0.008 (0.032)\tLoss 0.6706 (0.6910)\t\n","Test: [10/15]\tTime 0.006 (0.021)\tLoss 0.8747 (0.7070)\t\n","Acc: [tensor(82.5832)] F1score: 0.7832625245064577\n","[tensor(82.5832)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.151 (0.151)\tLoss 0.5208 (0.5208)\t\n","Test: [5/15]\tTime 0.012 (0.036)\tLoss 0.5106 (0.4703)\t\n","Test: [10/15]\tTime 0.004 (0.022)\tLoss 0.2772 (0.3886)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.187 (0.187)\tLoss 0.6975 (0.6975)\t\n","Test: [5/15]\tTime 0.016 (0.043)\tLoss 0.6370 (0.5072)\t\n","Test: [10/15]\tTime 0.004 (0.027)\tLoss 0.7850 (0.5463)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 13/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5166\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.150 (0.150)\tLoss 0.8674 (0.8674)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6674 (0.6979)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8725 (0.7097)\t\n","Acc: [tensor(81.8004)] F1score: 0.7689458442241541\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.149 (0.149)\tLoss 0.8382 (0.8382)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6339 (0.6513)\t\n","Test: [10/15]\tTime 0.006 (0.020)\tLoss 0.8139 (0.6675)\t\n","Acc: [tensor(82.9746)] F1score: 0.7881330295737282\n","[tensor(82.9746)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.145 (0.145)\tLoss 0.5235 (0.5235)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.5134 (0.4725)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.2782 (0.3902)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.144 (0.144)\tLoss 0.6882 (0.6882)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6267 (0.5010)\t\n","Test: [10/15]\tTime 0.007 (0.020)\tLoss 0.7722 (0.5389)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 14/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5739\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.146 (0.146)\tLoss 0.8517 (0.8517)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.6418 (0.6608)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8334 (0.6777)\t\n","Acc: [tensor(81.9961)] F1score: 0.7702445117682264\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.221 (0.221)\tLoss 0.8938 (0.8938)\t\n","Test: [5/15]\tTime 0.009 (0.047)\tLoss 0.6849 (0.7216)\t\n","Test: [10/15]\tTime 0.006 (0.029)\tLoss 0.9120 (0.7313)\t\n","Acc: [tensor(81.4090)] F1score: 0.7615044828868527\n","[tensor(81.4090)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.209 (0.209)\tLoss 0.5261 (0.5261)\t\n","Test: [5/15]\tTime 0.009 (0.043)\tLoss 0.5162 (0.4746)\t\n","Test: [10/15]\tTime 0.006 (0.027)\tLoss 0.2792 (0.3918)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.225 (0.225)\tLoss 0.6838 (0.6838)\t\n","Test: [5/15]\tTime 0.009 (0.046)\tLoss 0.6219 (0.4981)\t\n","Test: [10/15]\tTime 0.006 (0.028)\tLoss 0.7663 (0.5355)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 15/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5322\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.206 (0.206)\tLoss 0.8867 (0.8867)\t\n","Test: [5/15]\tTime 0.009 (0.047)\tLoss 0.6859 (0.7217)\t\n","Test: [10/15]\tTime 0.006 (0.029)\tLoss 0.9032 (0.7300)\t\n","Acc: [tensor(81.4090)] F1score: 0.7639769376483295\n","--- adapter training cost 0.024 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.141 (0.141)\tLoss 0.8442 (0.8442)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.6336 (0.6608)\t\n","Test: [10/15]\tTime 0.005 (0.020)\tLoss 0.8312 (0.6750)\t\n","Acc: [tensor(81.8004)] F1score: 0.7586751044959414\n","[tensor(81.8004)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.157 (0.157)\tLoss 0.5289 (0.5289)\t\n","Test: [5/15]\tTime 0.007 (0.035)\tLoss 0.5192 (0.4770)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.2802 (0.3936)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.148 (0.148)\tLoss 0.6802 (0.6802)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6180 (0.4958)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.7614 (0.5326)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 16/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4870\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.146 (0.146)\tLoss 0.8221 (0.8221)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6152 (0.6371)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.7971 (0.6522)\t\n","Acc: [tensor(82.7789)] F1score: 0.7778994000129029\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.145 (0.145)\tLoss 0.8668 (0.8668)\t\n","Test: [5/15]\tTime 0.007 (0.031)\tLoss 0.6618 (0.6876)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8783 (0.7023)\t\n","Acc: [tensor(81.6047)] F1score: 0.7627561772865099\n","[tensor(81.6047)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.161 (0.161)\tLoss 0.5319 (0.5319)\t\n","Test: [5/15]\tTime 0.007 (0.035)\tLoss 0.5223 (0.4795)\t\n","Test: [10/15]\tTime 0.004 (0.022)\tLoss 0.2812 (0.3954)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.147 (0.147)\tLoss 0.6763 (0.6763)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6136 (0.4932)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.7560 (0.5295)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 17/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5280\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.146 (0.146)\tLoss 0.8195 (0.8195)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6196 (0.6487)\t\n","Test: [10/15]\tTime 0.006 (0.020)\tLoss 0.8127 (0.6608)\t\n","Acc: [tensor(81.9961)] F1score: 0.7626289227560172\n","--- adapter training cost 0.016 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.142 (0.142)\tLoss 0.8542 (0.8542)\t\n","Test: [5/15]\tTime 0.009 (0.033)\tLoss 0.6546 (0.6844)\t\n","Test: [10/15]\tTime 0.007 (0.022)\tLoss 0.8688 (0.6962)\t\n","Acc: [tensor(81.8004)] F1score: 0.7586751044959414\n","[tensor(81.8004)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.229 (0.229)\tLoss 0.5345 (0.5345)\t\n","Test: [5/15]\tTime 0.009 (0.050)\tLoss 0.5250 (0.4816)\t\n","Test: [10/15]\tTime 0.006 (0.030)\tLoss 0.2822 (0.3970)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.200 (0.200)\tLoss 0.6749 (0.6749)\t\n","Test: [5/15]\tTime 0.009 (0.045)\tLoss 0.6121 (0.4923)\t\n","Test: [10/15]\tTime 0.006 (0.028)\tLoss 0.7543 (0.5285)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 18/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.5753\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.235 (0.235)\tLoss 0.8381 (0.8381)\t\n","Test: [5/15]\tTime 0.010 (0.052)\tLoss 0.6330 (0.6598)\t\n","Test: [10/15]\tTime 0.006 (0.032)\tLoss 0.8443 (0.6743)\t\n","Acc: [tensor(82.1918)] F1score: 0.761094097787604\n","--- adapter training cost 0.025 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.144 (0.144)\tLoss 0.8565 (0.8565)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6566 (0.6851)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8697 (0.6976)\t\n","Acc: [tensor(81.6047)] F1score: 0.7627561772865099\n","[tensor(81.6047)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.167 (0.167)\tLoss 0.5373 (0.5373)\t\n","Test: [5/15]\tTime 0.008 (0.035)\tLoss 0.5280 (0.4840)\t\n","Test: [10/15]\tTime 0.004 (0.022)\tLoss 0.2831 (0.3987)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.156 (0.156)\tLoss 0.6713 (0.6713)\t\n","Test: [5/15]\tTime 0.007 (0.033)\tLoss 0.6082 (0.4900)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.7494 (0.5257)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 19/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4752\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.150 (0.150)\tLoss 0.8248 (0.8248)\t\n","Test: [5/15]\tTime 0.007 (0.032)\tLoss 0.6274 (0.6544)\t\n","Test: [10/15]\tTime 0.004 (0.020)\tLoss 0.8248 (0.6670)\t\n","Acc: [tensor(81.8004)] F1score: 0.7613954264066086\n","--- adapter training cost 0.017 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.153 (0.153)\tLoss 0.8519 (0.8519)\t\n","Test: [5/15]\tTime 0.007 (0.034)\tLoss 0.6398 (0.6731)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8590 (0.6859)\t\n","Acc: [tensor(82.1918)] F1score: 0.761094097787604\n","[tensor(82.1918)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.168 (0.168)\tLoss 0.5404 (0.5404)\t\n","Test: [5/15]\tTime 0.007 (0.036)\tLoss 0.5312 (0.4867)\t\n","Test: [10/15]\tTime 0.005 (0.023)\tLoss 0.2841 (0.4005)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.165 (0.165)\tLoss 0.6670 (0.6670)\t\n","Test: [5/15]\tTime 0.007 (0.035)\tLoss 0.6034 (0.4872)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.7435 (0.5223)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","\n","===> epoch: 20/20\n","current lr 1.00000e-04\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","Training adapter:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:379: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  t_output = F.softmax(t_output / args.T)\n","/content/drive/.shortcut-targets-by-id/1tKMR3GBF4sOow2QngUrbzr2dSCsBTZib/MetaTeacher/train.py:195: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  l1 = nn.KLDivLoss()(F.log_softmax(clone_y / T), clone_weighted_logits) * (T * T * 2.0 * alpha)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","epoch[0/1]adapter Loss: 0.4833\n","adapter.theta=tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n","         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n","         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n","          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n","          1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,  0.1530, -0.4757,\n","         -0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n","         -0.9274,  0.5451,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n","          0.1374,  0.9386, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479],\n","        [-0.0255, -1.0233, -0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,\n","         -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,\n","         -0.5065,  0.0998, -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600,\n","          0.2546, -0.5020, -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907,\n","          0.6614,  1.1899,  0.8165, -0.9135,  1.3851, -0.8138, -0.9276,  1.1120,\n","          1.3352,  0.6043, -0.1034, -0.1512, -2.1021, -0.6200, -1.4782, -1.1334,\n","          0.8738, -0.5603,  1.2858,  0.8168,  0.2053,  0.3051,  0.5357, -0.4312,\n","          2.5581, -0.2334, -0.0135,  1.8606, -1.9804,  1.7986,  0.1018,  0.3400]],\n","       device='cuda:0', requires_grad=True),adapter.W=tensor([[ 7.1236e-01, -1.7765e+00],\n","        [ 3.5386e-01,  1.1996e+00],\n","        [-3.0300e-01, -1.7618e+00],\n","        [ 6.3484e-01, -8.0436e-01],\n","        [-1.6111e+00, -1.8716e+00],\n","        [ 5.4308e-01,  6.6068e-01],\n","        [ 2.2952e+00,  6.7491e-01],\n","        [ 1.7133e+00, -1.7943e+00],\n","        [-1.3633e+00, -9.8322e-01],\n","        [ 1.5113e+00,  6.4187e-01],\n","        [ 4.7296e-01, -4.2859e-01],\n","        [ 5.5137e-01, -1.5474e+00],\n","        [ 5.1811e-01,  1.0654e-01],\n","        [ 2.6924e-01,  1.3248e+00],\n","        [ 1.7460e+00,  1.8550e+00],\n","        [-7.0637e-01,  2.5571e+00],\n","        [ 4.1753e-01, -2.1272e-01],\n","        [-8.3996e-01, -4.2002e-01],\n","        [-6.2404e-01, -9.7730e-01],\n","        [ 8.7484e-01,  9.8728e-01],\n","        [ 3.0958e-01,  1.5207e+00],\n","        [ 1.2052e+00, -1.8156e+00],\n","        [-4.0346e-01, -9.5915e-01],\n","        [-5.2077e-03, -7.8863e-02],\n","        [ 8.4365e-01,  1.1657e+00],\n","        [ 5.2693e-01,  1.6193e+00],\n","        [-9.6398e-01,  1.4152e-01],\n","        [-1.6366e-01, -3.5822e-01],\n","        [ 1.7223e+00, -3.0358e-01],\n","        [ 2.3887e-01,  1.3440e+00],\n","        [ 1.0323e-01,  1.1004e+00],\n","        [-3.4168e-01,  9.4734e-01],\n","        [-5.6852e-01,  8.3760e-01],\n","        [ 1.7837e+00, -1.9542e-01],\n","        [ 5.1492e-01, -1.8475e+00],\n","        [-2.9167e+00, -5.6733e-01],\n","        [-5.4128e-01,  8.9517e-01],\n","        [-8.8251e-01,  5.3181e-01],\n","        [-1.5458e+00, -1.7330e-01],\n","        [ 7.2825e-01,  5.7061e-02],\n","        [ 9.0552e-01,  1.0463e+00],\n","        [-5.2060e-01,  1.3548e+00],\n","        [ 2.3519e-01,  1.9142e+00],\n","        [ 1.8364e+00,  1.3245e+00],\n","        [-9.6901e-01,  1.2516e+00],\n","        [ 1.2103e+00, -5.2792e-01],\n","        [ 2.1857e-01, -5.7431e-01],\n","        [ 1.4571e+00,  1.7710e+00],\n","        [ 1.6499e+00, -4.3200e-01],\n","        [-2.7103e-01, -1.4392e+00],\n","        [ 1.2470e+00,  1.2739e+00],\n","        [ 3.9095e-01,  3.8721e-01],\n","        [-7.9829e-02,  3.4172e-01],\n","        [ 9.4883e-01, -1.3839e+00],\n","        [ 1.7241e+00, -2.3648e+00],\n","        [-9.2949e-01,  2.9363e-01],\n","        [ 2.1513e-01,  9.3846e-01],\n","        [ 1.4657e+00, -5.5647e-01],\n","        [-7.4484e-01, -2.0216e-01],\n","        [-2.2967e-01,  1.3313e-03],\n","        [ 3.7528e-01, -5.8107e-01],\n","        [-5.7231e-01,  1.0097e+00],\n","        [-1.0565e-01, -1.1797e+00],\n","        [-9.0780e-02,  5.6311e-01]], device='cuda:0', requires_grad=True)\n","Testing:\n","Test: [0/15]\tTime 0.147 (0.147)\tLoss 0.8034 (0.8034)\t\n","Test: [5/15]\tTime 0.009 (0.032)\tLoss 0.6091 (0.6351)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.7967 (0.6474)\t\n","Acc: [tensor(81.9961)] F1score: 0.7626289227560172\n","--- adapter training cost 0.017 mins ---\n","Training:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing:\n","Test: [0/15]\tTime 0.163 (0.163)\tLoss 0.8239 (0.8239)\t\n","Test: [5/15]\tTime 0.007 (0.034)\tLoss 0.6226 (0.6489)\t\n","Test: [10/15]\tTime 0.004 (0.021)\tLoss 0.8261 (0.6627)\t\n","Acc: [tensor(82.1918)] F1score: 0.761094097787604\n","[tensor(82.1918)]\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.158 (0.158)\tLoss 0.5426 (0.5426)\t\n","Test: [5/15]\tTime 0.007 (0.035)\tLoss 0.5336 (0.4885)\t\n","Test: [10/15]\tTime 0.004 (0.022)\tLoss 0.2850 (0.4019)\t\n","Acc: [tensor(86.3014)] F1score: 0.8726060283539521\n","Testing:\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Test: [0/15]\tTime 0.179 (0.179)\tLoss 0.6679 (0.6679)\t\n","Test: [5/15]\tTime 0.009 (0.041)\tLoss 0.6044 (0.4878)\t\n","Test: [10/15]\tTime 0.007 (0.026)\tLoss 0.7448 (0.5230)\t\n","Acc: [tensor(81.0176)] F1score: 0.7511328641015164\n","BEST ACC: 82.975\n","--- 1.538 mins ---\n"]}],"source":["!python train.py"],"id":"ROBK6-6QRlkw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3obC30Lm2HC"},"outputs":[],"source":[],"id":"S3obC30Lm2HC"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":66.111703,"end_time":"2022-08-23T17:40:55.963410","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-23T17:39:49.851707","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}